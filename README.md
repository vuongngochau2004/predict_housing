# ðŸ  PredictHousing - Real Estate Data Crawler

A professional web scraping tool for collecting real estate data from **Nhatot.com** (Vietnam's leading classifieds platform) using Selenium and BeautifulSoup.

## ðŸ“‹ Features

- âœ… **Automated data collection** from nhatot.com real estate listings
- âœ… **Semantic HTML parsing** using `itemprop` attributes for reliable extraction
- âœ… **Retry mechanism** with automatic browser restart on failures
- âœ… **Headless mode** for faster performance
- âœ… **Auto-save** functionality to prevent data loss
- âœ… **Comprehensive field extraction**: Price, Location, Area, Property Type, Legal Documents, Direction, Dimensions, Rooms, Floors, Furnishing
- âœ… **Beautiful logging** with Loguru
- âœ… **Pagination support** with smart empty-page detection

## ðŸš€ Quick Start

### Prerequisites

```bash
pip install loguru pandas selenium beautifulsoup4 webdriver-manager
```

### Basic Usage

```bash
python crawl_nhatot.py
```

The script will:
1. Crawl up to 100 pages from nhatot.com
2. Extract all listing URLs
3. Parse detailed information from each listing
4. Save data to `nhatot_real_estate.csv`
5. Auto-save every 5 records

## ðŸ“Š Data Fields

The following fields are extracted from each listing:

| Field | Description | Example |
|-------|-------------|---------|
| **GiÃ¡ bÃ¡n** | Sale price | "15,5 tá»·" |
| **ThÃ nh phá»‘** | City/Province | "Tp Há»“ ChÃ­ Minh" |
| **PhÆ°á»ng/XÃ£** | District/Ward | "Quáº­n 1" |
| **Diá»‡n tÃ­ch (m2)** | Land area | "68" |
| **Loáº¡i hÃ¬nh** | Property type | "NhÃ  máº·t phá»‘, máº·t tiá»n" |
| **Giáº¥y tá» phÃ¡p lÃ½** | Legal documents | "ÄÃ£ cÃ³ sá»•" |
| **HÆ°á»›ng** | Direction | "ÄÃ´ng Nam" |
| **Chiá»u ngang (m)** | Width | "7" |
| **Chiá»u dÃ i (m)** | Length | "10" |
| **Sá»‘ phÃ²ng ngá»§** | Bedrooms | "3" |
| **Sá»‘ phÃ²ng vá»‡ sinh** | Bathrooms | "2" |
| **Sá»‘ táº§ng** | Floors | "2" |
| **TÃ¬nh tráº¡ng ná»™i tháº¥t** | Furnishing status | "HoÃ n thiá»‡n cÆ¡ báº£n" |

## ðŸ› ï¸ Configuration

### Adjust Crawling Speed

Edit `crawl_nhatot.py`:

```python
# Line 72: Listing pages wait time
self._random_sleep(4, 6)  # Increase for more reliability, decrease for speed

# Line 148: Detail pages wait time  
self._random_sleep(2, 3)  # Adjust based on your connection speed
```

### Headless Mode

```python
# Line 34: Toggle headless mode
chrome_options.add_argument("--headless=new")  # Comment out to see browser
```

### Max Pages

```python
# Line 343: Set maximum pages to crawl
scraper.run("https://www.nhatot.com/mua-ban-nha-dat", pages=100)
```

## ðŸ”§ Technical Details

### Architecture

- **Selenium WebDriver**: Handles JavaScript-rendered pages
- **BeautifulSoup**: Parses HTML and extracts data
- **Loguru**: Beautiful logging with colors and timestamps
- **Pandas**: Data manipulation and CSV export

### Anti-Detection Features

1. **Stealth settings**: Removes automation flags
2. **Random sleep intervals**: Mimics human behavior
3. **User-Agent rotation**: Appears as legitimate browser
4. **CDP commands**: Hides `navigator.webdriver` property

### Error Handling

- **Browser crash recovery**: Auto-restarts driver and retries (max 3 attempts)
- **Keyboard interrupt**: Saves data before exiting (Ctrl+C)
- **Empty page detection**: Stops after 3 consecutive empty pages
- **Per-record error handling**: Continues crawling even if individual pages fail

## ðŸ“ˆ Performance

### Speed Optimization

| Configuration | Speed | Reliability |
|---------------|-------|-------------|
| Default (headless + optimized timing) | ~2.5s per listing | â­â­â­â­â­ |
| Non-headless | ~5s per listing | â­â­â­â­â­ |
| Very fast (2s sleep) | ~1.5s per listing | â­â­â­ (may miss data) |

**Estimated time for 100 listings**: ~5-7 minutes

## ðŸ“ Output Format

CSV file with UTF-8-BOM encoding (Excel-compatible):

```csv
GiÃ¡ bÃ¡n,ThÃ nh phá»‘,PhÆ°á»ng/XÃ£,Diá»‡n tÃ­ch (m2),Loáº¡i hÃ¬nh,...
"15,5 tá»·",Tp Há»“ ChÃ­ Minh,Quáº­n 1,68,"NhÃ  máº·t phá»‘, máº·t tiá»n",...
"1,7 tá»·",BÃ¬nh DÆ°Æ¡ng,PhÆ°á»ng BÃ¬nh HÃ²a,60,"NhÃ  ngÃµ, háº»m",...
```

## ðŸ› Troubleshooting

### Issue: "No URLs found on page 2, 3, 4"

**Solution**: JavaScript didn't load in time. Increase wait time:
```python
self._random_sleep(5, 8)  # Line 72
```

### Issue: "Browser crashed" errors

**Solution**: Automatically handled by retry mechanism. If persists:
1. Update Chrome browser
2. Update chromedriver: `pip install --upgrade webdriver-manager`

### Issue: "Extracted 0/13 fields"

**Cause**: Page structure changed or Cloudflare blocked request
**Solution**: 
1. Check if website is accessible manually
2. May need to update selectors if site redesigned

## ðŸ”’ Legal & Ethical Considerations

- âš ï¸ **Respect robots.txt**: Check website's crawling policy
- âš ï¸ **Rate limiting**: Built-in delays to avoid server overload
- âš ï¸ **Data usage**: For educational/research purposes only
- âš ï¸ **Terms of Service**: Ensure compliance with nhatot.com's ToS

## ðŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- [ ] Multi-threading support
- [ ] Database storage (PostgreSQL/MongoDB)
- [ ] More detailed error reporting Dashboard
- [ ] Export to multiple formats (JSON, Excel, SQLite)

## ðŸ“ License

This project is for educational purposes. Please ensure compliance with local laws and website terms of service before use.

## ðŸ‘¨â€ðŸ’» Author

**DUT-AI Team**  
University Project - Data Science & Machine Learning

---

### ðŸ“š Dependencies

```txt
loguru>=0.7.0
pandas>=2.0.0
selenium>=4.0.0
beautifulsoup4>=4.12.0
webdriver-manager>=4.0.0
```

### ðŸ”— Related Projects

- Data Analysis: Coming soon
- Price Prediction Model: Coming soon
- Web Dashboard: Coming soon
