# üìä Gi·∫£i Th√≠ch Chi Ti·∫øt: HOW & WHY X·ª≠ L√Ω D·ªØ Li·ªáu

> **T√†i li·ªáu n√†y gi·∫£i th√≠ch QUY TR√åNH v√† L√ù DO ƒë·∫±ng sau m·ªói b∆∞·ªõc x·ª≠ l√Ω d·ªØ li·ªáu**

---

## üìã M·ª•c L·ª•c

1. [T·ªïng Quan Pipeline](#1-t·ªïng-quan-pipeline)
2. [Log Transform](#2-log-transform---chi-ti·∫øt)
3. [Outlier Detection](#3-outlier-detection---chi-ti·∫øt)
4. [Missing Value Imputation](#4-missing-value-imputation---chi-ti·∫øt)
5. [Feature Engineering](#5-feature-engineering---chi-ti·∫øt)
6. [Encoding](#6-encoding---chi-ti·∫øt)
7. [Train/Test Split](#7-traintestsplit---chi-ti·∫øt)

---

## 1. T·ªïng Quan Pipeline

### **Quy tr√¨nh x·ª≠ l√Ω (7 b∆∞·ªõc)**

```
Raw Data (19,733 rows)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 1: CLEANING                                            ‚îÇ
‚îÇ - Remove empty rows         (13,888 rows removed)           ‚îÇ
‚îÇ - Parse Vietnamese price    ("3,5 t·ª∑" ‚Üí 3,500,000,000)      ‚îÇ
‚îÇ - Clean string in numeric   ("nhi·ªÅu h∆°n 10" ‚Üí 10)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (5,845 rows)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 2: OUTLIER REMOVAL                                     ‚îÇ
‚îÇ - Domain bounds            (3 outliers)                     ‚îÇ
‚îÇ - IQR method               (345 outliers)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì (5,497 rows)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 3: MISSING VALUE IMPUTATION                            ‚îÇ
‚îÇ - Categorical ‚Üí "Kh√¥ng x√°c ƒë·ªãnh"                            ‚îÇ
‚îÇ - Numeric ‚Üí Median (global ho·∫∑c group)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 4: FEATURE ENGINEERING                                 ‚îÇ
‚îÇ - Gi√°_per_m2, T·ªïng_ph√≤ng, Aspect_ratio, Di·ªán_t√≠ch_per_ph√≤ng ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 5: ENCODING                                            ‚îÇ
‚îÇ - One-Hot (19 columns)                                      ‚îÇ
‚îÇ - Target Encoding (2 columns)                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 6: LOG TRANSFORM                                       ‚îÇ
‚îÇ - Gi√° b√°n, Di·ªán t√≠ch, Gi√°_per_m2                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STEP 7: TRAIN/TEST SPLIT                                    ‚îÇ
‚îÇ - 80% Train (4,397)                                         ‚îÇ
‚îÇ - 20% Test (1,100)                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Model-Ready Data (5,497 rows, 34 features)
```

---

## 2. Log Transform - Chi Ti·∫øt

### ‚ùì **WHY: T·∫°i sao ph·∫£i d√πng Log Transform?**

#### **V·∫•n ƒë·ªÅ v·ªõi d·ªØ li·ªáu gi√° nh√†:**

```
Gi√° nh√† th·ª±c t·∫ø:
‚îú‚îÄ‚îÄ 500 tri·ªáu - 5 t·ª∑:     ~60% (nhi·ªÅu)
‚îú‚îÄ‚îÄ 5 t·ª∑ - 15 t·ª∑:         ~30% (trung b√¨nh)
‚îú‚îÄ‚îÄ 15 t·ª∑ - 50 t·ª∑:        ~8%  (√≠t)
‚îî‚îÄ‚îÄ 50 t·ª∑ - 200 t·ª∑:       ~2%  (r·∫•t √≠t - bi·ªát th·ª±)
```

Ph√¢n ph·ªëi n√†y g·ªçi l√† **RIGHT-SKEWED (l·ªách ph·∫£i)** v√¨:
- **ƒêu√¥i d√†i b√™n ph·∫£i**: V√†i cƒÉn nh√† r·∫•t ƒë·∫Øt k√©o d√†i bi·ªÉu ƒë·ªì
- **Mean > Median**: 7.7 t·ª∑ > 5.9 t·ª∑ (mean b·ªã k√©o b·ªüi outliers)
- **Skewness = 49.54**: C·ª±c k·ª≥ l·ªách (b√¨nh th∆∞·ªùng < 1)

#### **T·∫°i sao ƒëi·ªÅu n√†y l√† v·∫•n ƒë·ªÅ?**

1. **Linear Regression gi·∫£ ƒë·ªãnh residuals tu√¢n theo ph√¢n ph·ªëi chu·∫©n (Normal Distribution)**
   ```
   N·∫øu Y (gi√°) kh√¥ng normal ‚Üí residuals kh√¥ng normal ‚Üí model sai
   ```

2. **Outliers c√≥ ·∫£nh h∆∞·ªüng kh√¥ng c√¢n ƒë·ªëi**
   ```
   Nh√† 100 t·ª∑ ·∫£nh h∆∞·ªüng model g·∫•p 20 l·∫ßn nh√† 5 t·ª∑
   ‚Üí Model c·ªë fit outliers thay v√¨ ƒëa s·ªë d·ªØ li·ªáu
   ```

3. **Scale kh√°c nhau qu√° l·ªõn**
   ```
   500 tri·ªáu vs 100 t·ª∑ = ch√™nh l·ªách 200 l·∫ßn
   ‚Üí Model kh√≥ h·ªçc pattern
   ```

---

### üîß **HOW: Log Transform ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?**

#### **C√¥ng th·ª©c:**
```python
# Log1p = log(1 + x) ƒë·ªÉ tr√°nh log(0) = undefined
df['Gi√° b√°n_log'] = np.log1p(df['Gi√° b√°n_numeric'])
```

#### **V√≠ d·ª• c·ª• th·ªÉ:**

| Gi√° g·ªëc (VND) | Log(Gi√°) | Gi·∫£i th√≠ch |
|---------------|----------|------------|
| 500 tri·ªáu     | 20.03    | log1p(500,000,000) |
| 1 t·ª∑          | 20.72    | TƒÉng g·∫•p ƒë√¥i ‚Üí ch·ªâ +0.69 |
| 5 t·ª∑          | 22.33    | TƒÉng 10x ‚Üí ch·ªâ +2.3 |
| 10 t·ª∑         | 23.03    | TƒÉng 20x ‚Üí ch·ªâ +3.0 |
| 50 t·ª∑         | 24.63    | TƒÉng 100x ‚Üí ch·ªâ +4.6 |
| 100 t·ª∑        | 25.33    | TƒÉng 200x ‚Üí ch·ªâ +5.3 |

**Nh·∫≠n x√©t:** Sau log, ch√™nh l·ªách 500 tri·ªáu ‚Üî 100 t·ª∑ ch·ªâ c√≤n ~5 ƒë∆°n v·ªã thay v√¨ 200x

#### **K·∫øt qu·∫£:**
```
TR∆Ø·ªöC log:
- Skewness = 49.54 (c·ª±c k·ª≥ l·ªách)
- Range: 0 - 1,250 t·ª∑

SAU log:
- Skewness = -0.44 (g·∫ßn symmetric!)
- Range: 18.9 - 23.8 (ch·ªâ ~5 ƒë∆°n v·ªã)
```

---

#### **Visualization so s√°nh:**

```
TR∆Ø·ªöC LOG (Original):                 SAU LOG:
                                      
    ‚îÇ                                     ‚îÇ    ‚ï≠‚îÄ‚ïÆ
    ‚îÇ                                     ‚îÇ   ‚ï≠‚ïØ ‚ï∞‚ïÆ
    ‚îÇ                                     ‚îÇ  ‚ï≠‚ïØ   ‚ï∞‚ïÆ
    ‚îÇ‚ï≠‚ïÆ                                   ‚îÇ ‚ï≠‚ïØ     ‚ï∞‚ïÆ
    ‚ï∞‚ïØ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫           ‚ï∞‚îÄ‚ïØ       ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫
    
    ƒêu√¥i d√†i b√™n ph·∫£i                     Symmetric (h√¨nh chu√¥ng)
    Outliers r√µ r√†ng                      Outliers gi·∫£m ƒë√°ng k·ªÉ
```

---

### ‚ö†Ô∏è **L∆ØU √ù QUAN TR·ªåNG:**

#### **Khi predict, ph·∫£i convert ng∆∞·ª£c:**
```python
# Model predict ra log scale
y_pred_log = model.predict(X_test)

# Convert v·ªÅ VND th·ª±c
y_pred_vnd = np.expm1(y_pred_log)  # expm1 = exp(x) - 1 (ng∆∞·ª£c c·ªßa log1p)

# V√≠ d·ª•:
# y_pred_log = 22.33 ‚Üí y_pred_vnd = 5,000,000,000 (5 t·ª∑)
```

---

## 3. Outlier Detection - Chi Ti·∫øt

### ‚ùì **WHY: T·∫°i sao ph·∫£i x·ª≠ l√Ω Outliers?**

#### **Outliers l√† g√¨?**
C√°c ƒëi·ªÉm d·ªØ li·ªáu **b·∫•t th∆∞·ªùng** so v·ªõi ph·∫ßn c√≤n l·∫°i:
- Nh√† gi√° 500 t·ª∑ trong khi ƒëa s·ªë < 20 t·ª∑
- Di·ªán t√≠ch 5000m¬≤ trong khi ƒëa s·ªë < 200m¬≤
- Gi√°/m¬≤ = 5 t·ª∑/m¬≤ (r√µ r√†ng sai)

#### **Ngu·ªìn g·ªëc outliers:**
1. **L·ªói nh·∫≠p li·ªáu**: Nh·∫≠p sai s·ªë (thi·∫øu s·ªë 0, th·ª´a s·ªë 0)
2. **ƒê∆°n v·ªã kh√°c**: Nh·∫ßm tri·ªáu v·ªõi t·ª∑
3. **Outliers th·∫≠t**: Bi·ªát th·ª±, penthouse (real nh∆∞ng rare)
4. **Gian l·∫≠n**: Gi√° ·∫£o ƒë·ªÉ SEO/marketing

#### **T·∫°i sao ph·∫£i x·ª≠ l√Ω?**

1. **Model b·ªã distort:**
   ```
   Mean B·ªä K√âO:
   - Kh√¥ng outlier: Mean = 5 t·ª∑ (ƒë√∫ng)
   - C√≥ 1 outlier 500 t·ª∑: Mean = 10 t·ª∑ (sai!)
   ```

2. **Linear Regression r·∫•t sensitive:**
   ```
   RSS = Œ£(y - ≈∑)¬≤
   
   Outlier 500 t·ª∑ sai 100 t·ª∑:
   (100 t·ª∑)¬≤ = 10,000 t·ª∑¬≤ ‚Üí ·∫£nh h∆∞·ªüng c·ª±c l·ªõn!
   ```

3. **Overfitting ƒë·∫øn outliers:**
   ```
   Model c·ªë h·ªçc pattern c·ªßa outliers
   ‚Üí B·ªè qua pattern c·ªßa ƒëa s·ªë 99% data
   ```

---

### üîß **HOW: X·ª≠ l√Ω Outliers nh∆∞ th·∫ø n√†o?**

#### **2 ph∆∞∆°ng ph√°p k·∫øt h·ª£p:**

### **Ph∆∞∆°ng ph√°p 1: Domain Knowledge Bounds (Ki·∫øn th·ª©c nghi·ªáp v·ª•)**

```python
OUTLIER_BOUNDS = {
    'Gi√° b√°n_numeric': (100_000_000, 500_000_000_000),
    # 100 tri·ªáu - 500 t·ª∑
    # Gi·∫£i th√≠ch:
    # - < 100 tri·ªáu: Kh√¥ng th·ªÉ l√† nh√† (ch·∫Øc l√† ƒë·∫•t ho·∫∑c l·ªói)
    # - > 500 t·ª∑: Qu√° ƒë·∫Øt, c√≥ th·ªÉ ·∫£o ho·∫∑c r·∫•t rare
    
    'Di·ªán t√≠ch (m2)': (5, 10000),
    # 5m¬≤ - 10,000m¬≤ (1 hectare)
    # Gi·∫£i th√≠ch:
    # - < 5m¬≤: Kh√¥ng th·ªÉ l√† nh√† ·ªü
    # - > 10,000m¬≤: ƒê·∫•t d·ª± √°n, kh√¥ng ph·∫£i nh√†
    
    'Gi√°_per_m2': (1_000_000, 1_000_000_000),
    # 1 tri·ªáu/m¬≤ - 1 t·ª∑/m¬≤
    # Gi·∫£i th√≠ch:
    # - < 1 tri·ªáu/m¬≤: Qu√° r·∫ª, ch·∫Øc l·ªói
    # - > 1 t·ª∑/m¬≤: Kh√¥ng h·ª£p l√Ω (ƒë·∫Øt nh·∫•t VN ~300tr/m¬≤)
}

# Code:
for col, (lower, upper) in OUTLIER_BOUNDS.items():
    df = df[(df[col] >= lower) & (df[col] <= upper)]
```

**K·∫øt qu·∫£:** Lo·∫°i 3 outliers r√µ r√†ng sai

---

### **Ph∆∞∆°ng ph√°p 2: IQR Method (Th·ªëng k√™)**

#### **IQR l√† g√¨?**
```
IQR = InterQuartile Range = Q3 - Q1

Q1 (25th percentile): 25% data nh·ªè h∆°n gi√° tr·ªã n√†y
Q3 (75th percentile): 75% data nh·ªè h∆°n gi√° tr·ªã n√†y
IQR: Kho·∫£ng ch·ª©a 50% data ·ªü gi·ªØa
```

#### **C√¥ng th·ª©c ph√°t hi·ªán outlier:**
```
Lower bound = Q1 - k √ó IQR
Upper bound = Q3 + k √ó IQR

N·∫øu x < Lower bound ho·∫∑c x > Upper bound ‚Üí Outlier
```

#### **T·∫°i sao ch·ªçn k = 3.0 thay v√¨ 1.5?**

| k | T√™n g·ªçi | ∆Øu ƒëi·ªÉm | Nh∆∞·ª£c ƒëi·ªÉm |
|---|---------|---------|------------|
| **1.5** | Standard | Lo·∫°i nhi·ªÅu outlier | M·∫•t data th·∫≠t (villa) |
| **3.0** | Conservative | Gi·ªØ data, ch·ªâ lo·∫°i extreme | C√≤n 1 s·ªë outlier |

**V·ªõi b·∫•t ƒë·ªông s·∫£n, k=3.0 l√† ƒê√öNG v√¨:**
```
Nh√† 50 t·ª∑:
- C√≥ TH·ªÇ l√† bi·ªát th·ª± cao c·∫•p (REAL)
- k=1.5: B·ªã lo·∫°i ‚Üí M·∫§T DATA T·ªêT
- k=3.0: Gi·ªØ l·∫°i ‚Üí ƒê√öN'G

Nh√† 500 t·ª∑:
- Ch·∫Øc ch·∫Øn b·∫•t th∆∞·ªùng (l·ªói ho·∫∑c si√™u outlier)
- C·∫£ k=1.5 v√† k=3.0 ƒë·ªÅu lo·∫°i ‚Üí ƒê√ö'NG
```

#### **Code implementation:**
```python
def remove_outliers_iqr(df, column, multiplier=3.0):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - multiplier * IQR
    upper_bound = Q3 + multiplier * IQR
    
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
```

**K·∫øt qu·∫£:** Lo·∫°i 345 outliers (6.3% data)

---

#### **V√≠ d·ª• th·ª±c t·∫ø v·ªõi gi√° nh√†:**

```
D·ªØ li·ªáu gi√° (t·ª∑ VND):
[1.2, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 7.0, 8.0, 10.0, 15.0, 50.0, 200.0]

Q1 = 3.25 t·ª∑
Q3 = 8.5 t·ª∑
IQR = 5.25 t·ª∑

k = 1.5:
- Lower = 3.25 - 1.5√ó5.25 = -4.6 t·ª∑ (d∆∞·ªõi 0, kh√¥ng √°p d·ª•ng)
- Upper = 8.5 + 1.5√ó5.25 = 16.4 t·ª∑
- ‚Üí Lo·∫°i: 50 t·ª∑, 200 t·ª∑ ‚úì
- ‚Üí Nh∆∞ng c≈©ng lo·∫°i 15 t·ª∑ (c√≥ th·ªÉ real!)

k = 3.0:
- Lower = 3.25 - 3√ó5.25 = -12.5 t·ª∑
- Upper = 8.5 + 3√ó5.25 = 24.25 t·ª∑
- ‚Üí Lo·∫°i: 50 t·ª∑, 200 t·ª∑ ‚úì
- ‚Üí Gi·ªØ: 15 t·ª∑ (c√≥ th·ªÉ real) ‚úì
```

---

## 4. Missing Value Imputation - Chi Ti·∫øt

### ‚ùì **WHY: T·∫°i sao ph·∫£i fill missing values?**

1. **Nhi·ªÅu ML algorithms kh√¥ng ch·∫•p nh·∫≠n NaN:**
   ```python
   sklearn: ValueError: Input contains NaN
   ```

2. **N·∫øu drop t·∫•t c·∫£ missing ‚Üí m·∫•t qu√° nhi·ªÅu data:**
   ```
   C·ªôt "H∆∞·ªõng" missing 70%
   Drop all ‚Üí m·∫•t 70% dataset!
   ```

3. **Missing c√≥ th·ªÉ ch·ª©a th√¥ng tin:**
   ```
   "H∆∞·ªõng" = missing c√≥ th·ªÉ nghƒ©a: "Ng∆∞·ªùi b√°n kh√¥ng quan t√¢m h∆∞·ªõng"
   ‚Üí ƒê√¢y l√† th√¥ng tin h·ªØu √≠ch!
   ```

---

### üîß **HOW: X·ª≠ l√Ω Missing nh∆∞ th·∫ø n√†o?**

#### **3 chi·∫øn l∆∞·ª£c kh√°c nhau cho 3 lo·∫°i c·ªôt:**

### **Chi·∫øn l∆∞·ª£c 1: Fill "Kh√¥ng x√°c ƒë·ªãnh" cho Categorical**

**√Åp d·ª•ng cho:** `H∆∞·ªõng`, `T√¨nh tr·∫°ng n·ªôi th·∫•t`

```python
df['H∆∞·ªõng'].fillna('Kh√¥ng x√°c ƒë·ªãnh', inplace=True)
```

**T·∫°i sao kh√¥ng d√πng Mode (gi√° tr·ªã ph·ªï bi·∫øn nh·∫•t)?**
```
Mode c·ªßa H∆∞·ªõng = "ƒê√¥ng Nam"

N·∫øu fill Mode:
- T·∫•t c·∫£ missing ‚Üí "ƒê√¥ng Nam"
- Model h·ªçc: "ƒê√¥ng Nam" = ph·ªï bi·∫øn (SAI!)
- Th·ª±c t·∫ø: Nhi·ªÅu nh√† kh√¥ng bi·∫øt h∆∞·ªõng

N·∫øu fill "Kh√¥ng x√°c ƒë·ªãnh":
- Model h·ªçc: C√≥ 1 category ri√™ng cho "kh√¥ng bi·∫øt"
- N·∫øu "kh√¥ng bi·∫øt h∆∞·ªõng" ·∫£nh h∆∞·ªüng gi√° ‚Üí model s·∫Ω h·ªçc ƒë∆∞·ª£c!
```

---

### **Chi·∫øn l∆∞·ª£c 2: Fill Median theo Group cho Numeric quan tr·ªçng**

**√Åp d·ª•ng cho:** `Chi·ªÅu ngang`, `Chi·ªÅu d√†i`, `S·ªë t·∫ßng`

```python
df['Chi·ªÅu ngang (m)'] = df.groupby('Lo·∫°i h√¨nh')['Chi·ªÅu ngang (m)'].transform(
    lambda x: x.fillna(x.median())
)
```

**T·∫°i sao Group theo "Lo·∫°i h√¨nh"?**
```
Lo·∫°i h√¨nh = "Bi·ªát th·ª±":
- Chi·ªÅu ngang trung b√¨nh: 10m
- N√™n fill missing = 10m

Lo·∫°i h√¨nh = "Nh√† ng√µ, h·∫ªm":
- Chi·ªÅu ngang trung b√¨nh: 4m
- N√™n fill missing = 4m

‚Üí Group gi√∫p fill PH√ô H·ª¢P v·ªõi t·ª´ng lo·∫°i nh√†!
```

**T·∫°i sao d√πng Median thay v√¨ Mean?**
```
D·ªØ li·ªáu chi·ªÅu ngang: [3, 4, 4, 5, 5, 5, 6, 20]

Mean = 6.5m (b·ªã k√©o b·ªüi 20m outlier)
Median = 5m (ƒë√∫ng v·ªõi ƒëa s·ªë)

‚Üí Median ROBUST h∆°n v·ªõi outliers!
```

---

### **Chi·∫øn l∆∞·ª£c 3: Fill Global Median cho Numeric √≠t quan tr·ªçng**

**√Åp d·ª•ng cho:** `S·ªë ph√≤ng ng·ªß`, `S·ªë ph√≤ng v·ªá sinh`

```python
df['S·ªë ph√≤ng v·ªá sinh'].fillna(df['S·ªë ph√≤ng v·ªá sinh'].median(), inplace=True)
```

**T·∫°i sao d√πng Global thay v√¨ Group?**
```
S·ªë ph√≤ng v·ªá sinh kh√¥ng ph·ª• thu·ªôc nhi·ªÅu v√†o Lo·∫°i h√¨nh:
- Bi·ªát th·ª±: 3-5 WC
- Nh√† ph·ªë: 2-3 WC
- Nh√† ng√µ: 1-2 WC

S·ª± kh√°c bi·ªát kh√¥ng qu√° l·ªõn ‚Üí Global median ƒë·ªß t·ªët
V√† ƒë∆°n gi·∫£n h∆°n Group median
```

---

#### **B·∫£ng t·ªïng h·ª£p:**

| C·ªôt | Strategy | L√Ω do |
|-----|----------|-------|
| **H∆∞·ªõng** | "Kh√¥ng x√°c ƒë·ªãnh" | Categorical, missing = th√¥ng tin |
| **N·ªôi th·∫•t** | "Kh√¥ng x√°c ƒë·ªãnh" | Categorical, missing = th√¥ng tin |
| **Chi·ªÅu ngang** | Group median | Ph·ª• thu·ªôc Lo·∫°i h√¨nh m·∫°nh |
| **Chi·ªÅu d√†i** | Group median | Ph·ª• thu·ªôc Lo·∫°i h√¨nh m·∫°nh |
| **S·ªë t·∫ßng** | Group median | Bi·ªát th·ª± vs Nh√† ng√µ kh√°c nhau |
| **S·ªë ph√≤ng ng·ªß** | Global median | Kh√¥ng kh√°c bi·ªát nhi·ªÅu gi·ªØa groups |
| **S·ªë ph√≤ng v·ªá sinh** | Global median | Kh√¥ng kh√°c bi·ªát nhi·ªÅu gi·ªØa groups |

---

## 5. Feature Engineering - Chi Ti·∫øt

### ‚ùì **WHY: T·∫°i sao ph·∫£i t·∫°o features m·ªõi?**

**"Raw features rarely tell the full story"**

#### **V√≠ d·ª• c·ª• th·ªÉ:**

```
Nh√† A: 50m¬≤, gi√° 5 t·ª∑, ·ªü Qu·∫≠n 1 HCM
Nh√† B: 200m¬≤, gi√° 5 t·ª∑, ·ªü ƒê·ªìng Nai

Ch·ªâ nh√¨n "Gi√°" v√† "Di·ªán t√≠ch" ri√™ng l·∫ª:
- C·∫£ 2 ƒë·ªÅu 5 t·ª∑ ‚Üí gi·ªëng nhau?
- A: 50m¬≤, B: 200m¬≤ ‚Üí B r·ªông h∆°n?

Nh√¨n "Gi√°/m¬≤":
- A: 100 tri·ªáu/m¬≤ ‚Üí R·∫∫ cho Q1 (avg = 200tr/m¬≤)
- B: 25 tri·ªáu/m¬≤ ‚Üí ƒê·∫ÆT cho ƒê·ªìng Nai (avg = 20tr/m¬≤)

‚Üí Gi√°/m¬≤ = INSIGHT TH·ª∞C S·ª∞!
```

---

### üîß **HOW: T·∫°o Features nh∆∞ th·∫ø n√†o?**

### **Feature 1: Gi√°_per_m2 (Gi√° tr√™n m√©t vu√¥ng)**

```python
df['Gi√°_per_m2'] = df['Gi√° b√°n_numeric'] / df['Di·ªán t√≠ch (m2)']
```

**T·∫°i sao quan tr·ªçng nh·∫•t?**
- Correlation v·ªõi gi√°: **+0.349** (cao nh·∫•t!)
- Chu·∫©n h√≥a gi√° theo k√≠ch th∆∞·ªõc
- ƒê·∫°i di·ªán cho "gi√° tr·ªã v·ªã tr√≠" (HCM > T√¢y Ninh)

---

### **Feature 2: T·ªïng_ph√≤ng (T·ªïng s·ªë ph√≤ng)**

```python
df['T·ªïng_ph√≤ng'] = df['S·ªë ph√≤ng ng·ªß'].fillna(0) + df['S·ªë ph√≤ng v·ªá sinh'].fillna(0)
```

**T·∫°i sao h·ªØu √≠ch?**
- Ph·∫£n √°nh **quy m√¥ t·ªïng th·ªÉ** c·ªßa nh√†
- Nhi·ªÅu ph√≤ng ‚Üí nh√† to ‚Üí gi√° cao h∆°n
- ƒê∆°n gi·∫£n h√≥a 2 features th√†nh 1

---

### **Feature 3: Aspect_ratio (T·ª∑ l·ªá h√¨nh d·∫°ng)**

```python
df['Aspect_ratio'] = df['Chi·ªÅu ngang (m)'] / df['Chi·ªÅu d√†i (m)']
```

**√ù nghƒ©a:**
```
Aspect_ratio ‚âà 1.0: Nh√† vu√¥ng (square)
Aspect_ratio < 0.5: Nh√† d√†i, h·∫πp (long, narrow)

V√≠ d·ª•:
- 5m x 10m ‚Üí ratio = 0.5 (h∆°i d√†i)
- 4m x 20m ‚Üí ratio = 0.2 (r·∫•t d√†i, gi√° th·∫•p h∆°n)
- 10m x 10m ‚Üí ratio = 1.0 (vu√¥ng, gi√° cao h∆°n)
```

**T·∫°i sao ·∫£nh h∆∞·ªüng gi√°?**
- Nh√† vu√¥ng d·ªÖ thi·∫øt k·∫ø n·ªôi th·∫•t
- Nh√† d√†i h·∫πp √°nh s√°ng k√©m
- M·∫∑t ti·ªÅn r·ªông (ratio cao) gi√° cao h∆°n

---

### **Feature 4: Di·ªán_t√≠ch_per_ph√≤ng (Di·ªán t√≠ch m·ªói ph√≤ng)**

```python
df['Di·ªán_t√≠ch_per_ph√≤ng'] = df['Di·ªán t√≠ch (m2)'] / df['T·ªïng_ph√≤ng']
```

**√ù nghƒ©a:**
- Cao = ph√≤ng r·ªông r√£i (spacious)
- Th·∫•p = ph√≤ng ch·∫≠t (cramped)

**·∫¢nh h∆∞·ªüng:**
- 60m¬≤ / 3 ph√≤ng = 20m¬≤/ph√≤ng (r·ªông r√£i)
- 60m¬≤ / 6 ph√≤ng = 10m¬≤/ph√≤ng (ch·∫≠t)
- C√πng di·ªán t√≠ch nh∆∞ng gi√° tr·ªã kh√°c nhau!

---

## 6. Encoding - Chi Ti·∫øt

### ‚ùì **WHY: T·∫°i sao ph·∫£i encode?**

**ML models ch·ªâ hi·ªÉu s·ªë, kh√¥ng hi·ªÉu text:**
```python
# Model nh√¨n th·∫•y:
"ƒê√¥ng Nam", "T√¢y B·∫Øc", "B·∫Øc"  ‚Üí ‚ùå Kh√¥ng hi·ªÉu

# Sau encoding:
[1, 0, 0], [0, 1, 0], [0, 0, 1]  ‚Üí ‚úÖ Hi·ªÉu ƒë∆∞·ª£c
```

---

### üîß **HOW: Encode nh∆∞ th·∫ø n√†o?**

### **Ph∆∞∆°ng ph√°p 1: One-Hot Encoding cho Low Cardinality**

**√Åp d·ª•ng cho:** `Lo·∫°i h√¨nh` (5 values), `H∆∞·ªõng` (9 values), `N·ªôi th·∫•t` (5 values)

```python
# T·ª´:
Lo·∫°i h√¨nh = ["Nh√† ph·ªë", "Bi·ªát th·ª±", "Nh√† ng√µ"]

# Th√†nh:
Lo·∫°i h√¨nh_Nh√† ph·ªë    Lo·∫°i h√¨nh_Bi·ªát th·ª±    Lo·∫°i h√¨nh_Nh√† ng√µ
      1                    0                     0
      0                    1                     0
      0                    0                     1
```

**T·∫°i sao ch·ªâ d√πng cho <10 categories?**
```
10 categories ‚Üí 10 columns m·ªõi (OK)
100 categories ‚Üí 100 columns (ch·∫•p nh·∫≠n ƒë∆∞·ª£c)
500 categories ‚Üí 500 columns (CURSE OF DIMENSIONALITY!)
```

---

### **Ph∆∞∆°ng ph√°p 2: Target Encoding cho High Cardinality**

**√Åp d·ª•ng cho:** `Ph∆∞·ªùng/X√£` (~500 values), `Th√†nh ph·ªë` (~30 values)

```python
# √ù t∆∞·ªüng:
# M·ªói category ‚Üí trung b√¨nh c·ªßa target (Gi√°) trong category ƒë√≥

# V√≠ d·ª•:
Ph∆∞·ªùng B·∫øn Ngh√© (Q1 HCM):
- C√°c nh√† ·ªü ƒë√¢y c√≥ gi√° trung b√¨nh: 15 t·ª∑
- ‚Üí encoded = 15,000,000,000

X√£ B√¨nh H∆∞ng (B√¨nh Ch√°nh):
- C√°c nh√† ·ªü ƒë√¢y c√≥ gi√° trung b√¨nh: 3 t·ª∑
- ‚Üí encoded = 3,000,000,000
```

**∆Øu ƒëi·ªÉm:**
- 500 categories ‚Üí ch·ªâ 1 column m·ªõi
- Gi·ªØ ƒë∆∞·ª£c th√¥ng tin v·ªÅ "gi√° tr·ªã location"
- Kh√¥ng explosion s·ªë features

**Nh∆∞·ª£c ƒëi·ªÉm:**
- C√≥ th·ªÉ data leakage n·∫øu kh√¥ng c·∫©n th·∫≠n

---

## 7. Train/Test Split - Chi Ti·∫øt

### ‚ùì **WHY: T·∫°i sao ph·∫£i split?**

**"You can't grade your own exam"**

#### **V·∫•n ƒë·ªÅ n·∫øu kh√¥ng split:**

```
Scenario: Train v√† Test tr√™n 100% data

B∆∞·ªõc 1: Model "h·ªçc thu·ªôc" to√†n b·ªô data
        ‚Üí Train accuracy = 99%

B∆∞·ªõc 2: Test tr√™n data ƒë√£ h·ªçc
        ‚Üí Test accuracy = 99% (t·∫•t nhi√™n!)

B∆∞·ªõc 3: Deploy l√™n production
        ‚Üí Real accuracy = 40% üíÄ

T·∫°i sao? OVERFITTING!
Model h·ªçc thu·ªôc noise, kh√¥ng h·ªçc pattern
```

---

### üîß **HOW: Split nh∆∞ th·∫ø n√†o?**

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 20% cho test
    random_state=42,    # Reproducible
    shuffle=True        # Random shuffle
)
```

**T·∫°i sao 80/20?**
- ƒê·ªß data ƒë·ªÉ train: 4,397 samples
- ƒê·ªß data ƒë·ªÉ test reliable: 1,100 samples
- Standard practice cho dataset >5000

**T·∫°i sao random_state=42?**
- Reproducibility: Ch·∫°y l·∫°i cho k·∫øt qu·∫£ gi·ªëng nhau
- Debug d·ªÖ h∆°n
- 42: S·ªë ph·ªï bi·∫øn (t·ª´ "Hitchhiker's Guide to Galaxy")

---

## üìä T·ªïng K·∫øt: Workflow Reasoning

| Step | Action | WHY | HOW |
|------|--------|-----|-----|
| **1. Clean** | Remove empty, parse text | Garbage in = garbage out | dropna(), custom parse |
| **2. Outliers** | Remove extreme values | Model sensitive to outliers | Domain + IQR√ó3 |
| **3. Missing** | Fill NaN | ML c·∫ßn numeric | Category‚Üítext, Numeric‚Üímedian |
| **4. Features** | Create new columns | Domain knowledge > raw | Division, sum operations |
| **5. Encode** | Convert text‚Üínumber | ML kh√¥ng hi·ªÉu text | One-hot, Target encoding |
| **6. Log** | Transform skewed | Normal assumption | np.log1p() |
| **7. Split** | Separate train/test | Avoid overfitting | train_test_split |

---

## üéØ Key Decisions Summary

| Decision | WHY | Alternative & Why Not |
|----------|-----|----------------------|
| **Log transform** | Skewness 49‚Üí0 | StandardScaler: kh√¥ng fix skew |
| **IQR√ó3** | Keep real expensive houses | IQR√ó1.5: lose too much data |
| **Group median** | Respect data patterns | Global median: ignore groups |
| **Target encoding** | 500 categories‚Üí1 column | One-hot: 500 columns |
| **80/20 split** | Industry standard | 90/10: test too small |

---

**M·ªçi quy·∫øt ƒë·ªãnh ƒë·ªÅu c√≥ L√ù DO v√† d·ª±a tr√™n EVIDENCE t·ª´ data!**
