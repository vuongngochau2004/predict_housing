# ğŸ“˜ TÃ i liá»‡u Ká»¹ thuáº­t: Training Model & Data Crawling

TÃ i liá»‡u nÃ y giáº£i thÃ­ch chi tiáº¿t vá» chiáº¿n lÆ°á»£c huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n giÃ¡ nhÃ  vÃ  quy trÃ¬nh thu tháº­p dá»¯ liá»‡u, bao gá»“m lÃ½ do (Why) vÃ  cÃ¡ch thá»©c thá»±c hiá»‡n (How) cho tá»«ng ká»¹ thuáº­t.

---

## ğŸ“‘ Má»¥c lá»¥c

1. [Thu tháº­p Dá»¯ liá»‡u (Data Crawling)](#1-thu-tháº­p-dá»¯-liá»‡u-data-crawling)
2. [Chiáº¿n lÆ°á»£c Huáº¥n luyá»‡n (Training Strategy)](#2-chiáº¿n-lÆ°á»£c-huáº¥n-luyá»‡n-training-strategy)
3. [Optuna - Tá»‘i Æ°u Hyperparameters](#3-optuna---tá»‘i-Æ°u-hyperparameters)
4. [K-Fold Cross Validation](#4-k-fold-cross-validation)

---

## 1. Thu tháº­p Dá»¯ liá»‡u (Data Crawling)

### ğŸ“ WHY - Táº¡i sao cáº§n crawl dá»¯ liá»‡u tá»« Nhatot.com?

1. **Nguá»“n dá»¯ liá»‡u thá»±c táº¿**: Nhatot.com lÃ  má»™t trong nhá»¯ng trang web mua bÃ¡n báº¥t Ä‘á»™ng sáº£n lá»›n nháº¥t Viá»‡t Nam, cung cáº¥p dá»¯ liá»‡u thá»±c vá» giÃ¡ nhÃ  Ä‘áº¥t.

2. **Dá»¯ liá»‡u phong phÃº**: Má»—i tin Ä‘Äƒng chá»©a nhiá»u Ä‘áº·c trÆ°ng quan trá»ng:
   - GiÃ¡ bÃ¡n
   - Diá»‡n tÃ­ch, chiá»u ngang, chiá»u dÃ i
   - Vá»‹ trÃ­ (ThÃ nh phá»‘, PhÆ°á»ng/XÃ£)
   - Loáº¡i hÃ¬nh báº¥t Ä‘á»™ng sáº£n
   - Sá»‘ phÃ²ng ngá»§, sá»‘ phÃ²ng vá»‡ sinh, sá»‘ táº§ng
   - Giáº¥y tá» phÃ¡p lÃ½, hÆ°á»›ng, tÃ¬nh tráº¡ng ná»™i tháº¥t

3. **KhÃ´ng cÃ³ API cÃ´ng khai**: Nhatot.com khÃ´ng cung cáº¥p API Ä‘á»ƒ láº¥y dá»¯ liá»‡u, do Ä‘Ã³ cáº§n pháº£i crawl trá»±c tiáº¿p tá»« trang web.

### ğŸ”§ HOW - CÃ¡ch thá»©c crawl dá»¯ liá»‡u

#### CÃ´ng nghá»‡ sá»­ dá»¥ng

| CÃ´ng nghá»‡ | Má»¥c Ä‘Ã­ch |
|-----------|----------|
| **Crawl4AI** | Framework async crawler hiá»‡n Ä‘áº¡i vá»›i browser automation |
| **BeautifulSoup** | Parse HTML Ä‘á»ƒ trÃ­ch xuáº¥t thÃ´ng tin |
| **asyncio** | Xá»­ lÃ½ báº¥t Ä‘á»“ng bá»™, tÄƒng tá»‘c Ä‘á»™ crawl |

#### CÃ¡c ká»¹ thuáº­t chÃ­nh

**1. Async Crawling vá»›i Browser Pooling**

```python
async with AsyncWebCrawler(config=browser_config) as crawler:
    # Crawl nhiá»u trang Ä‘á»“ng thá»i
    tasks = [self.parse_detail_page(crawler, url) for url in batch]
    results = await asyncio.gather(*tasks, return_exceptions=True)
```

> **Why?** Sá»­ dá»¥ng async giÃºp crawl Ä‘á»“ng thá»i nhiá»u trang, giáº£m thá»i gian chá» tá»« hÃ ng giá» xuá»‘ng cÃ²n vÃ i phÃºt.

**2. Stealth Mode - Cháº¿ Ä‘á»™ áº©n danh**

```python
browser_config = BrowserConfig(
    headless=True,
    user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...",
    extra_args=[
        "--disable-blink-features=AutomationControlled",
        "--disable-dev-shm-usage",
    ],
    use_managed_browser=True,
)
```

> **Why?** TrÃ¡nh bá»‹ phÃ¡t hiá»‡n lÃ  bot bá»Ÿi anti-bot systems, giáº£ láº­p hÃ nh vi trÃ¬nh duyá»‡t tháº­t.

**3. JSON-LD Extraction**

```python
json_scripts = soup.find_all('script', type='application/ld+json')
for script in json_scripts:
    data = json.loads(script.string)
    if data.get('@type') == 'ItemList':
        # Extract listing URLs
```

> **Why?** JSON-LD lÃ  structured data Ä‘Æ°á»£c nhÃºng sáºµn trong HTML, dá»… parse vÃ  á»•n Ä‘á»‹nh hÆ¡n so vá»›i selector CSS.

**4. Concurrency Control**

```python
MAX_CONCURRENT = 10  # Sá»‘ trang crawl Ä‘á»“ng thá»i
for i in range(0, len(listing_urls), self.max_concurrent):
    batch = listing_urls[i:i + self.max_concurrent]
    # Process batch
```

> **Why?** Kiá»ƒm soÃ¡t sá»‘ lÆ°á»£ng request Ä‘á»“ng thá»i Ä‘á»ƒ khÃ´ng gÃ¢y quÃ¡ táº£i server vÃ  trÃ¡nh bá»‹ cháº·n IP.

**5. Periodic Saving**

```python
if (i + self.max_concurrent) % 20 == 0:
    self._save_to_csv()
```

> **Why?** LÆ°u dá»¯ liá»‡u Ä‘á»‹nh ká»³ Ä‘á»ƒ trÃ¡nh máº¥t dá»¯ liá»‡u náº¿u crawler gáº·p lá»—i giá»¯a chá»«ng.

### ğŸ“Š Káº¿t quáº£

- **Input**: URL trang tÃ¬m kiáº¿m Nhatot.com (`https://www.nhatot.com/mua-ban-nha-dat`)
- **Output**: File CSV chá»©a thÃ´ng tin báº¥t Ä‘á»™ng sáº£n vá»›i 13 features
- **Hiá»‡u suáº¥t**: ~2-3 giÃ¢y/tin Ä‘Äƒng vá»›i 10 concurrent pages

---

## 2. Chiáº¿n lÆ°á»£c Huáº¥n luyá»‡n (Training Strategy)

### ğŸ“ WHY - Táº¡i sao cáº§n chiáº¿n lÆ°á»£c huáº¥n luyá»‡n Ä‘áº·c biá»‡t?

1. **So sÃ¡nh nhiá»u mÃ´ hÃ¬nh**: Sá»­ dá»¥ng 3 mÃ´ hÃ¬nh khÃ¡c nhau (LightGBM, RandomForest, CatBoost) Ä‘á»ƒ tÃ¬m ra mÃ´ hÃ¬nh tá»‘t nháº¥t.

2. **Tá»‘i Æ°u hyperparameters**: Má»—i mÃ´ hÃ¬nh cÃ³ nhiá»u hyperparameters cáº§n tinh chá»‰nh Ä‘á»ƒ Ä‘áº¡t hiá»‡u suáº¥t tá»‘t nháº¥t.

3. **ÄÃ¡nh giÃ¡ khÃ¡ch quan**: Cáº§n phÆ°Æ¡ng phÃ¡p Ä‘Ã¡nh giÃ¡ robust Ä‘á»ƒ Ä‘áº£m báº£o mÃ´ hÃ¬nh generalize tá»‘t.

### ğŸ”§ HOW - Pipeline huáº¥n luyá»‡n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PREPARATION                          â”‚
â”‚  â€¢ Load train/test data                                      â”‚
â”‚  â€¢ Clean feature names                                       â”‚
â”‚  â€¢ Convert categorical columns                               â”‚
â”‚  â€¢ Label encode for sklearn models                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 OPTUNA HYPERPARAMETER TUNING                 â”‚
â”‚  â€¢ 30 trials per model                                       â”‚
â”‚  â€¢ 5-Fold CV per trial                                       â”‚
â”‚  â€¢ Optimize for minimum RMSE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               K-FOLD CROSS VALIDATION                        â”‚
â”‚  â€¢ Train with optimized params                               â”‚
â”‚  â€¢ Compute CV metrics (RMSE, MAE, RÂ²)                        â”‚
â”‚  â€¢ Evaluate on test set                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               MODEL SELECTION & SAVING                       â”‚
â”‚  â€¢ Compare models, select best                               â”‚
â”‚  â€¢ Save all models and metadata                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“Š CÃ¡c mÃ´ hÃ¬nh sá»­ dá»¥ng

| Model | Library | Xá»­ lÃ½ Categorical |
|-------|---------|-------------------|
| **LightGBM** | `lightgbm` | Native `category` dtype |
| **RandomForest** | `sklearn` | Label Encoding |
| **CatBoost** | `catboost` | Native vá»›i `cat_features` |

---

## 3. Optuna - Tá»‘i Æ°u Hyperparameters

### ğŸ“ WHY - Táº¡i sao sá»­ dá»¥ng Optuna?

1. **Hiá»‡u quáº£ cao hÆ¡n Grid Search**: Grid Search thá»­ táº¥t cáº£ tá»• há»£p â†’ O(n^k). Optuna sá»­ dá»¥ng TPE (Tree-structured Parzen Estimator) thÃ´ng minh hÆ¡n.

2. **Tá»± Ä‘á»™ng pruning**: Dá»«ng sá»›m cÃ¡c trial kÃ©m hiá»‡u quáº£, tiáº¿t kiá»‡m thá»i gian.

3. **Dá»… Ä‘á»‹nh nghÄ©a search space**: API Ä‘Æ¡n giáº£n vá»›i `suggest_int`, `suggest_float`, `suggest_categorical`.

4. **TÃ­ch há»£p CV**: Káº¿t há»£p vá»›i Cross-Validation Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ má»—i trial.

### ğŸ”§ HOW - CÃ¡ch triá»ƒn khai Optuna

**1. Táº¡o Objective Function cho má»—i mÃ´ hÃ¬nh**

```python
def create_lightgbm_objective(X_train, y_train, n_folds=5):
    def objective(trial):
        param = {
            'n_estimators': trial.suggest_int('n_estimators', 200, 2000),
            'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 20),
            # ... more params
        }
        
        model = LGBMRegressor(**param)
        scores = cross_val_score(model, X_train, y_train, cv=n_folds, 
                                 scoring='neg_root_mean_squared_error')
        return -scores.mean()  # Minimize RMSE
    
    return objective
```

**2. Sá»­ dá»¥ng TPE Sampler**

```python
sampler = TPESampler(seed=42)  # Reproducible
study = optuna.create_study(direction='minimize', sampler=sampler)
study.optimize(objective, n_trials=30, show_progress_bar=True)
```

> **Why TPE?** TPE sá»­ dá»¥ng Bayesian optimization, há»c tá»« cÃ¡c trial trÆ°á»›c Ä‘á»ƒ chá»n hyperparameters cho trial sau thÃ´ng minh hÆ¡n.

**3. CatBoost Native CV**

```python
# CatBoost cÃ³ CV function riÃªng, tá»‘i Æ°u hÆ¡n sklearn
cv_results = catboost_cv(
    pool=Pool(X_train, y_train, cat_features=cat_features),
    params=param,
    fold_count=5,
    early_stopping_rounds=50  # Dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n
)
```

### ğŸ“Š Search Space cho má»—i mÃ´ hÃ¬nh

| Model | Hyperparameter | Range | Type |
|-------|----------------|-------|------|
| **LightGBM** | n_estimators | 200 - 2000 | int |
| | learning_rate | 0.001 - 0.3 | log float |
| | max_depth | 3 - 20 | int |
| | num_leaves | 15 - 500 | int |
| **RandomForest** | n_estimators | 100 - 1000 | int |
| | max_depth | 5 - 30 | int |
| | max_features | sqrt, log2, None | categorical |
| **CatBoost** | iterations | 200 - 2000 | int |
| | depth | 4 - 12 | int |
| | l2_leaf_reg | 1e-8 - 100 | log float |

### ğŸ”„ Caching Hyperparameters

```python
# Náº¿u Ä‘Ã£ cÃ³ hyperparams tá»« láº§n cháº¡y trÆ°á»›c â†’ dÃ¹ng láº¡i
if os.path.exists('models/best_hyperparams.json'):
    best_params = json.load(f)
    print("âœ… Loaded cached hyperparameters")
else:
    # Cháº¡y Optuna optimization
    best_params, studies = optimize_all_models(...)
```

> **Why?** Tiáº¿t kiá»‡m thá»i gian khi re-train, chá»‰ cáº§n xÃ³a file JSON náº¿u muá»‘n tÃ¬m hyperparams má»›i.

---

## 4. K-Fold Cross Validation

### ğŸ“ WHY - Táº¡i sao sá»­ dá»¥ng K-Fold CV?

1. **ÄÃ¡nh giÃ¡ robust**: Thay vÃ¬ split cá»‘ Ä‘á»‹nh 1 láº§n, K-Fold CV Ä‘Ã¡nh giÃ¡ model K láº§n trÃªn K táº­p khÃ¡c nhau.

2. **Táº­n dá»¥ng toÃ n bá»™ dá»¯ liá»‡u**: Má»—i sample Ä‘Æ°á»£c dÃ¹ng lÃ m validation Ä‘Ãºng 1 láº§n.

3. **Æ¯á»›c lÆ°á»£ng variance**: Äá»™ lá»‡ch chuáº©n giá»¯a cÃ¡c fold cho biáº¿t model cÃ³ á»•n Ä‘á»‹nh khÃ´ng.

4. **TrÃ¡nh overfitting**: Model khÃ´ng thá»ƒ "nhá»›" validation set vÃ¬ má»—i fold cÃ³ validation khÃ¡c nhau.

### ğŸ”§ HOW - CÃ¡ch triá»ƒn khai K-Fold CV

**1. Cáº¥u hÃ¬nh K-Fold**

```python
N_FOLDS = 5
kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)
```

| Tham sá»‘ | GiÃ¡ trá»‹ | Giáº£i thÃ­ch |
|---------|---------|------------|
| `n_splits` | 5 | Chia dá»¯ liá»‡u thÃ nh 5 pháº§n |
| `shuffle` | True | XÃ¡o trá»™n dá»¯ liá»‡u trÆ°á»›c khi chia |
| `random_state` | 42 | Äáº£m báº£o reproducibility |

**2. Training Loop**

```python
cv_scores = {'rmse': [], 'mae': [], 'r2': []}

for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train), 1):
    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
    
    model = LGBMRegressor(**params)
    model.fit(X_fold_train, y_fold_train)
    
    y_pred = model.predict(X_fold_val)
    
    cv_scores['rmse'].append(np.sqrt(mean_squared_error(y_fold_val, y_pred)))
    cv_scores['r2'].append(r2_score(y_fold_val, y_pred))
    
print(f"Mean RÂ²: {np.mean(cv_scores['r2']):.4f} Â± {np.std(cv_scores['r2']):.4f}")
```

**3. Minh há»a K-Fold (K=5)**

```
Fold 1: [Val] [Train] [Train] [Train] [Train]
Fold 2: [Train] [Val] [Train] [Train] [Train]
Fold 3: [Train] [Train] [Val] [Train] [Train]
Fold 4: [Train] [Train] [Train] [Val] [Train]
Fold 5: [Train] [Train] [Train] [Train] [Val]
```

### ğŸ“Š So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

| PhÆ°Æ¡ng phÃ¡p | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|-------------|---------|------------|
| **Hold-out** | Nhanh | Káº¿t quáº£ phá»¥ thuá»™c vÃ o cÃ¡ch chia |
| **K-Fold CV** | Robust, Æ°á»›c lÆ°á»£ng variance | Cháº­m hÆ¡n K láº§n |
| **LOO (Leave-One-Out)** | DÃ¹ng tá»‘i Ä‘a dá»¯ liá»‡u | Ráº¥t cháº­m (N láº§n training) |
| **Stratified K-Fold** | Giá»¯ tá»· lá»‡ class | Chá»‰ dÃ¹ng cho classification |

> **Káº¿t luáº­n**: 5-Fold CV lÃ  lá»±a chá»n cÃ¢n báº±ng giá»¯a tá»‘c Ä‘á»™ vÃ  Ä‘á»™ tin cáº­y.

### ğŸ“ˆ Káº¿t quáº£ máº«u

```
ğŸ“Š Training LightGBM...
--------------------------------------------------
   Fold 1: RMSE=0.8234, RÂ²=0.8712
   Fold 2: RMSE=0.7891, RÂ²=0.8845
   Fold 3: RMSE=0.8012, RÂ²=0.8789
   Fold 4: RMSE=0.8456, RÂ²=0.8634
   Fold 5: RMSE=0.7923, RÂ²=0.8801
--------------------------------------------------
   Mean: RMSE=0.8103 Â± 0.0217
         RÂ²=0.8756 Â± 0.0075
```

---

## ğŸ“ Output Files

### Models (`models/`)

| File | MÃ´ táº£ |
|------|-------|
| `model.joblib` | Model tá»‘t nháº¥t (production) |
| `lightgbm_optuna_model.joblib` | LightGBM Ä‘Ã£ optimize |
| `randomforest_optuna_model.joblib` | RandomForest Ä‘Ã£ optimize |
| `catboost_optuna_model.joblib` | CatBoost Ä‘Ã£ optimize |
| `best_hyperparams.json` | Hyperparameters tá»‘i Æ°u |
| `cv_scores.json` | Äiá»ƒm CV cho tá»«ng fold |

### Visualizations (`outputs/`)

| File | MÃ´ táº£ |
|------|-------|
| `optuna_optimization_history.png` | QuÃ¡ trÃ¬nh tá»‘i Æ°u cá»§a Optuna |
| `model_comparison.png` | So sÃ¡nh RMSE/MAE/RÂ² |
| `cv_scores.png` | RÂ² theo tá»«ng fold |
| `training_summary.png` | Tá»•ng káº¿t training |

---

## ğŸš€ Sá»­ dá»¥ng

```bash
# 1. Crawl dá»¯ liá»‡u (náº¿u cáº§n)
python crawl_nhatot_crawl4ai.py

# 2. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
# (xem docs/data_processing_guide.md)

# 3. Huáº¥n luyá»‡n model
python src/train_model.py

# 4. Cháº¡y á»©ng dá»¥ng
streamlit run app.py
```

---

## ğŸ“š References

- [Optuna Documentation](https://optuna.readthedocs.io/)
- [Crawl4AI GitHub](https://github.com/unclecode/crawl4ai)
- [scikit-learn Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)
- [LightGBM Parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
- [CatBoost CV](https://catboost.ai/en/docs/concepts/python-reference_cv)
