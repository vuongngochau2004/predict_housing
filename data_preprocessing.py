"""
Data Preprocessing for Nhatot Housing Dataset
Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cho b·ªô d·ªØ li·ªáu b·∫•t ƒë·ªông s·∫£n Nhatot

This script handles:
- Loading data
- Cleaning empty rows and duplicates
- Handling missing values
- Encoding categorical variables
- Scaling numerical features
- Feature engineering
- Saving processed data
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')


class HousingDataPreprocessor:
    """
    Comprehensive preprocessor for housing data
    """
    
    def __init__(self, file_path):
        """
        Initialize preprocessor
        
        Args:
            file_path: Path to CSV file
        """
        self.file_path = file_path
        self.df = None
        self.df_original = None  # L∆∞u tr·ªØ d·ªØ li·ªáu g·ªëc
        self.df_processed = None
        self.label_encoders = {}
        self.scaler = None
        
    def load_data(self):
        """Load data from CSV"""
        print("üìÇ Loading data...")
        self.df = pd.read_csv(self.file_path)
        # T·∫°o b·∫£n sao c·ªßa d·ªØ li·ªáu g·ªëc ƒë·ªÉ b·∫£o to√†n
        self.df_original = self.df.copy()
        print(f"‚úì Loaded {len(self.df)} rows and {len(self.df.columns)} columns")
        print(f"‚úì Created backup of original data")
        print(f"\nColumns: {list(self.df.columns)}")
        return self
    
    def clean_empty_rows(self):
        """Remove completely empty rows"""
        print("\nüßπ Cleaning empty rows...")
        initial_count = len(self.df)
        # Remove rows where all columns are NaN
        self.df = self.df.dropna(how='all')
        removed = initial_count - len(self.df)
        print(f"‚úì Removed {removed} empty rows")
        print(f"  Remaining: {len(self.df)} rows")
        return self
    
    def remove_duplicates(self):
        """Remove duplicate rows"""
        print("\nüîç Removing duplicates...")
        initial_count = len(self.df)
        self.df = self.df.drop_duplicates()
        removed = initial_count - len(self.df)
        print(f"‚úì Removed {removed} duplicate rows")
        return self
    
    def analyze_missing_values(self):
        """Analyze missing values in dataset"""
        print("\nüìä Missing Values Analysis:")
        print("=" * 60)
        missing_stats = pd.DataFrame({
            'Column': self.df.columns,
            'Missing_Count': self.df.isnull().sum(),
            'Missing_Percentage': (self.df.isnull().sum() / len(self.df) * 100).round(2)
        })
        missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values(
            'Missing_Percentage', ascending=False
        )
        print(missing_stats.to_string(index=False))
        print("=" * 60)
        return self
    
    def parse_price(self, price_str):
        """
        Convert Vietnamese price format to numeric value
        
        Examples:
            "1,5 t·ª∑" -> 1500000000
            "500 tri·ªáu" -> 500000000
            "2,35 t·ª∑" -> 2350000000
        """
        if pd.isna(price_str):
            return np.nan
        
        price_str = str(price_str).strip()
        
        # Remove quotes if present
        price_str = price_str.replace('"', '')
        
        # Parse value
        try:
            if 't·ª∑' in price_str:
                # Billion VND
                value = price_str.replace('t·ª∑', '').strip()
                value = value.replace(',', '.')
                return float(value) * 1_000_000_000
            elif 'tri·ªáu' in price_str:
                # Million VND
                value = price_str.replace('tri·ªáu', '').strip()
                value = value.replace(',', '.')
                return float(value) * 1_000_000
            else:
                # Try direct conversion
                value = price_str.replace(',', '.')
                return float(value)
        except:
            return np.nan
    
    def clean_price_column(self):
        """Clean and convert price column"""
        print("\nüí∞ Processing price column...")
        self.df['Gi√° b√°n (VND)'] = self.df['Gi√° b√°n'].apply(self.parse_price)
        # Remove rows with invalid prices
        initial_count = len(self.df)
        self.df = self.df.dropna(subset=['Gi√° b√°n (VND)'])
        removed = initial_count - len(self.df)
        print(f"‚úì Converted prices to numeric")
        print(f"  Removed {removed} rows with invalid prices")
        print(f"  Price range: {self.df['Gi√° b√°n (VND)'].min():,.0f} - {self.df['Gi√° b√°n (VND)'].max():,.0f} VND")
        return self
    
    def parse_numeric_column(self, col_name):
        """Parse numeric columns that might have special values"""
        def parse_value(val):
            if pd.isna(val):
                return np.nan
            val_str = str(val).strip().lower()
            
            # Handle special values
            if 'nhi·ªÅu h∆°n' in val_str or 'h∆°n' in val_str:
                # Extract number if present
                import re
                numbers = re.findall(r'\d+', val_str)
                if numbers:
                    return float(numbers[0]) + 1  # Add 1 to represent "more than"
                return np.nan
            
            try:
                # Try direct conversion
                return float(val_str.replace(',', '.'))
            except:
                return np.nan
        
        self.df[col_name] = self.df[col_name].apply(parse_value)
    
    def clean_numeric_columns(self):
        """Clean all numeric columns"""
        print("\nüî¢ Processing numeric columns...")
        
        numeric_cols = [
            'Di·ªán t√≠ch (m2)',
            'Chi·ªÅu ngang (m)',
            'Chi·ªÅu d√†i (m)',
            'S·ªë ph√≤ng ng·ªß',
            'S·ªë ph√≤ng v·ªá sinh',
            'S·ªë t·∫ßng'
        ]
        
        for col in numeric_cols:
            if col in self.df.columns:
                self.parse_numeric_column(col)
                print(f"  ‚úì Cleaned {col}")
        
        return self
    
    def handle_missing_values(self, strategy='auto'):
        """
        Handle missing values with different strategies
        
        Args:
            strategy: 'auto', 'drop', or 'impute'
        """
        print(f"\nüîß Handling missing values (strategy: {strategy})...")
        
        if strategy == 'drop':
            # Drop rows with any missing values
            initial_count = len(self.df)
            self.df = self.df.dropna()
            removed = initial_count - len(self.df)
            print(f"  Removed {removed} rows with missing values")
        
        elif strategy == 'auto' or strategy == 'impute':
            # Impute missing values intelligently
            
            # Numeric columns: fill with median
            numeric_cols = self.df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if self.df[col].isnull().sum() > 0:
                    median_val = self.df[col].median()
                    self.df[col].fillna(median_val, inplace=True)
                    print(f"  ‚úì Filled {col} with median: {median_val:.2f}")
            
            # Categorical columns: fill with mode or 'Unknown'
            categorical_cols = self.df.select_dtypes(include=['object']).columns
            for col in categorical_cols:
                if col == 'Gi√° b√°n':  # Skip original price column
                    continue
                if self.df[col].isnull().sum() > 0:
                    mode_val = self.df[col].mode()
                    if len(mode_val) > 0:
                        self.df[col].fillna(mode_val[0], inplace=True)
                        print(f"  ‚úì Filled {col} with mode: {mode_val[0]}")
                    else:
                        self.df[col].fillna('Kh√¥ng r√µ', inplace=True)
                        print(f"  ‚úì Filled {col} with 'Kh√¥ng r√µ'")
        
        print(f"‚úì Missing values handled. Remaining rows: {len(self.df)}")
        return self
    
    def encode_categorical_features(self, method='label'):
        """
        Encode categorical features
        
        Args:
            method: 'label' for LabelEncoder, 'onehot' for One-Hot Encoding
        """
        print(f"\nüè∑Ô∏è  Encoding categorical features (method: {method})...")
        
        categorical_cols = [
            'Th√†nh ph·ªë',
            'Ph∆∞·ªùng/X√£',
            'Lo·∫°i h√¨nh',
            'Gi·∫•y t·ªù ph√°p l√Ω',
            'H∆∞·ªõng',
            'T√¨nh tr·∫°ng n·ªôi th·∫•t'
        ]
        
        if method == 'label':
            for col in categorical_cols:
                if col in self.df.columns:
                    le = LabelEncoder()
                    # Handle NaN by treating as a separate category
                    self.df[col] = self.df[col].fillna('Kh√¥ng r√µ')
                    self.df[f'{col}_encoded'] = le.fit_transform(self.df[col])
                    self.label_encoders[col] = le
                    n_categories = len(le.classes_)
                    print(f"  ‚úì Encoded {col} ({n_categories} categories)")
        
        elif method == 'onehot':
            # One-hot encoding
            for col in categorical_cols:
                if col in self.df.columns:
                    self.df[col] = self.df[col].fillna('Kh√¥ng r√µ')
                    dummies = pd.get_dummies(self.df[col], prefix=col, drop_first=True)
                    self.df = pd.concat([self.df, dummies], axis=1)
                    print(f"  ‚úì One-hot encoded {col} ({len(dummies.columns)} features)")
        
        return self
    
    def feature_engineering(self):
        """Create new features from existing ones"""
        print("\n‚öôÔ∏è  Feature Engineering...")
        
        # Price per square meter
        if 'Di·ªán t√≠ch (m2)' in self.df.columns and 'Gi√° b√°n (VND)' in self.df.columns:
            self.df['Gi√°/m2'] = self.df['Gi√° b√°n (VND)'] / self.df['Di·ªán t√≠ch (m2)']
            print("  ‚úì Created 'Gi√°/m2' (price per sqm)")
        
        # Total rooms
        if 'S·ªë ph√≤ng ng·ªß' in self.df.columns and 'S·ªë ph√≤ng v·ªá sinh' in self.df.columns:
            self.df['T·ªïng s·ªë ph√≤ng'] = self.df['S·ªë ph√≤ng ng·ªß'] + self.df['S·ªë ph√≤ng v·ªá sinh']
            print("  ‚úì Created 'T·ªïng s·ªë ph√≤ng' (total rooms)")
        
        # Area from dimensions
        if 'Chi·ªÅu ngang (m)' in self.df.columns and 'Chi·ªÅu d√†i (m)' in self.df.columns:
            self.df['Di·ªán t√≠ch ∆∞·ªõc t√≠nh'] = self.df['Chi·ªÅu ngang (m)'] * self.df['Chi·ªÅu d√†i (m)']
            print("  ‚úì Created 'Di·ªán t√≠ch ∆∞·ªõc t√≠nh' (estimated area)")
        
        # Property size category
        if 'Di·ªán t√≠ch (m2)' in self.df.columns:
            def categorize_size(area):
                if pd.isna(area):
                    return 'Kh√¥ng r√µ'
                if area < 30:
                    return 'R·∫•t nh·ªè'
                elif area < 50:
                    return 'Nh·ªè'
                elif area < 80:
                    return 'Trung b√¨nh'
                elif area < 150:
                    return 'L·ªõn'
                else:
                    return 'R·∫•t l·ªõn'
            
            self.df['K√≠ch th∆∞·ªõc'] = self.df['Di·ªán t√≠ch (m2)'].apply(categorize_size)
            print("  ‚úì Created 'K√≠ch th∆∞·ªõc' (size category)")
        
        return self
    
    def scale_features(self, method='standard', columns=None):
        """
        Scale numerical features
        
        Args:
            method: 'standard' or 'minmax'
            columns: List of columns to scale. If None, scale all numeric columns
        """
        print(f"\nüìè Scaling features (method: {method})...")
        
        if columns is None:
            # Select numeric columns (excluding encoded categorical and target)
            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()
            # Remove target and ID columns
            columns = [col for col in columns if 'Gi√° b√°n' not in col and '_encoded' not in col and col != 'Gi√°/m2']
        
        if method == 'standard':
            self.scaler = StandardScaler()
        elif method == 'minmax':
            self.scaler = MinMaxScaler()
        
        # Create scaled versions
        for col in columns:
            if col in self.df.columns:
                self.df[f'{col}_scaled'] = self.scaler.fit_transform(self.df[[col]])
                print(f"  ‚úì Scaled {col}")
        
        return self
    
    def get_processed_data(self):
        """Get the processed dataframe"""
        return self.df
    
    def save_processed_data(self, output_path=None, save_original=True):
        """
        Save processed data to CSV
        
        Args:
            output_path: Path for processed data file
            save_original: Whether to also save original data backup
        """
        if output_path is None:
            output_path = self.file_path.replace('.csv', '_processed.csv')
        
        # L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        print(f"\nüíæ Saving processed data...")
        self.df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"‚úì Processed data: {output_path}")
        print(f"  ‚Üí {len(self.df)} rows and {len(self.df.columns)} columns")
        
        # L∆∞u b·∫£n sao d·ªØ li·ªáu g·ªëc (n·∫øu c·∫ßn)
        if save_original and self.df_original is not None:
            original_backup_path = self.file_path.replace('.csv', '_original_backup.csv')
            self.df_original.to_csv(original_backup_path, index=False, encoding='utf-8-sig')
            print(f"\n‚úì Original data backup: {original_backup_path}")
            print(f"  ‚Üí {len(self.df_original)} rows and {len(self.df_original.columns)} columns")
        
        print(f"\nüìÅ Files saved:")
        print(f"  ‚Ä¢ Original file (unchanged): {self.file_path}")
        print(f"  ‚Ä¢ Processed file (new): {output_path}")
        if save_original and self.df_original is not None:
            print(f"  ‚Ä¢ Backup file (new): {original_backup_path}")
        
        return output_path
    
    def get_summary_statistics(self):
        """Print summary statistics"""
        print("\nüìà Summary Statistics:")
        print("=" * 80)
        
        # Numeric columns summary
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        print("\nNumeric Features:")
        print(self.df[numeric_cols].describe())
        
        # Categorical columns summary
        categorical_cols = self.df.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            print("\n\nCategorical Features:")
            for col in categorical_cols[:5]:  # Show first 5
                print(f"\n{col}:")
                print(self.df[col].value_counts().head())
        
        print("=" * 80)
        return self
    
    def prepare_for_modeling(self, target_col='Gi√° b√°n (VND)', test_size=0.2, random_state=42):
        """
        Prepare data for machine learning
        
        Args:
            target_col: Target column name
            test_size: Test set size
            random_state: Random seed
            
        Returns:
            X_train, X_test, y_train, y_test
        """
        print(f"\nüéØ Preparing data for modeling...")
        
        # Select feature columns (encoded and numeric)
        feature_cols = []
        for col in self.df.columns:
            if '_encoded' in col or '_scaled' in col:
                feature_cols.append(col)
            elif col in ['Di·ªán t√≠ch (m2)', 'Chi·ªÅu ngang (m)', 'Chi·ªÅu d√†i (m)', 
                         'S·ªë ph√≤ng ng·ªß', 'S·ªë ph√≤ng v·ªá sinh', 'S·ªë t·∫ßng', 'T·ªïng s·ªë ph√≤ng']:
                feature_cols.append(col)
        
        X = self.df[feature_cols].fillna(0)
        y = self.df[target_col]
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        print(f"‚úì Train set: {len(X_train)} samples")
        print(f"‚úì Test set: {len(X_test)} samples")
        print(f"‚úì Features: {len(feature_cols)}")
        
        return X_train, X_test, y_train, y_test


# Main execution
if __name__ == "__main__":
    # Initialize preprocessor
    preprocessor = HousingDataPreprocessor('nhatot_crawl4ai.csv')
    
    # Step 1: Load and clean data
    preprocessor.load_data()
    preprocessor.clean_empty_rows()
    preprocessor.remove_duplicates()
    
    # Step 2: Analyze missing values
    preprocessor.analyze_missing_values()
    
    # Step 3: Clean and parse columns
    preprocessor.clean_price_column()
    preprocessor.clean_numeric_columns()
    
    # Step 4: Handle missing values
    preprocessor.handle_missing_values(strategy='auto')
    
    # Step 5: Feature engineering
    preprocessor.feature_engineering()
    
    # Step 6: Encode categorical features
    preprocessor.encode_categorical_features(method='label')
    
    # Step 7: Scale numerical features
    # preprocessor.scale_features(method='standard')
    
    # Step 8: Save processed data
    output_file = preprocessor.save_processed_data()
    
    # Step 9: Show summary
    preprocessor.get_summary_statistics()
    
    # Step 10: Prepare for modeling (optional)
    print("\n" + "=" * 80)
    print("üéì Data is ready for modeling!")
    print("=" * 80)
    
    # Example: prepare train/test split
    try:
        X_train, X_test, y_train, y_test = preprocessor.prepare_for_modeling()
        print(f"\nFeature columns: {list(X_train.columns)}")
    except Exception as e:
        print(f"\nNote: {str(e)}")
    
    print(f"\n‚úÖ Preprocessing complete!")
    print(f"   Output file: {output_file}")
